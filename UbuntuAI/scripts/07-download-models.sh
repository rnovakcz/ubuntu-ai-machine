#!/bin/bash
#===============================================================================
# 07-download-models.sh - Sta쬰n칤 lok치ln칤ch model콢 (bez placen칳ch 칰캜t콢)
# Ubuntu 25.10 AI Development Environment
#===============================================================================

set -e

BLUE='\033[0;34m'; GREEN='\033[0;32m'; YELLOW='\033[1;33m'; NC='\033[0m'
log() { echo -e "${BLUE}[INFO]${NC} $1"; }
ok() { echo -e "${GREEN}[OK]${NC} $1"; }
warn() { echo -e "${YELLOW}[!]${NC} $1"; }

USER_REAL=${SUDO_USER:-$USER}
HOME_REAL=$(getent passwd "$USER_REAL" | cut -d: -f6)
MODELS_DIR="$HOME_REAL/ai-workspace/models"

log "========== STAHOV츼N칈 LOK츼LN칈CH MODEL콡 =========="
warn "Toto st치hne n캩kolik GB dat!"

mkdir -p "$MODELS_DIR"/{ollama,huggingface,embeddings}

#===============================================================================
# OLLAMA MODELY (vy쬬duje b캩쮂셖칤 ollama)
#===============================================================================
log "Stahov치n칤 Ollama model콢..."

if command -v ollama &> /dev/null; then
    # Z치kladn칤 LLM modely
    ollama pull llama3.2          # 2GB - v코eobecn칳, rychl칳
    ollama pull mistral           # 4GB - kvalitn칤, rychl칳
    ollama pull phi3              # 2GB - Microsoft, dobr칳 na k칩d
    ollama pull gemma2:2b         # 1.5GB - Google, mal칳 a rychl칳
    
    # K칩dov치n칤
    ollama pull codellama         # 4GB - Meta, specializovan칳 na k칩d
    ollama pull deepseek-coder:6.7b  # 4GB - DeepSeek, v칳born칳 na k칩d
    
    # Embeddings (pro RAG)
    ollama pull nomic-embed-text  # 275MB - embeddings
    ollama pull mxbai-embed-large # 670MB - lep코칤 embeddings
    
    # Vision (multimodal)
    ollama pull llava:7b          # 4.5GB - vision + text
    
    ollama list
    ok "Ollama modely sta쬰ny"
else
    warn "Ollama nen칤 nainstalov치na, p콏eskakuji"
fi

#===============================================================================
# HUGGING FACE MODELY (Python)
#===============================================================================
log "Stahov치n칤 Hugging Face model콢..."

if [ -d "$HOME_REAL/mambaforge" ]; then
    sudo -u "$USER_REAL" bash << 'HFMODELS'
source ~/mambaforge/etc/profile.d/conda.sh
conda activate ai

python << 'PYTHON'
from huggingface_hub import snapshot_download
import os

cache_dir = os.path.expanduser("~/ai-workspace/models/huggingface")
os.makedirs(cache_dir, exist_ok=True)

models = [
    # Text classification
    "distilbert-base-uncased-finetuned-sst-2-english",
    
    # Text generation (mal칠)
    "distilgpt2",
    "microsoft/phi-2",
    
    # Question answering
    "distilbert-base-cased-distilled-squad",
    
    # NER
    "dslim/bert-base-NER",
    
    # Sentence embeddings
    "sentence-transformers/all-MiniLM-L6-v2",
    "sentence-transformers/all-mpnet-base-v2",
    
    # Fill mask
    "distilbert-base-uncased",
    
    # Translation (mal칠)
    "Helsinki-NLP/opus-mt-en-cs",
    "Helsinki-NLP/opus-mt-cs-en",
]

print("Stahov치n칤 Hugging Face model콢...")
for model in models:
    try:
        print(f"  Downloading: {model}")
        snapshot_download(model, cache_dir=cache_dir)
    except Exception as e:
        print(f"  Error: {e}")

print("Hotovo!")
PYTHON
HFMODELS
    ok "Hugging Face modely sta쬰ny"
else
    warn "Mambaforge nen칤 nainstalov치n, p콏eskakuji HF modely"
fi

#===============================================================================
# WHISPER MODELY
#===============================================================================
log "Stahov치n칤 Whisper model콢..."

if [ -d "$HOME_REAL/mambaforge" ]; then
    sudo -u "$USER_REAL" bash << 'WHISPER'
source ~/mambaforge/etc/profile.d/conda.sh
conda activate ai

python << 'PYTHON'
# Faster Whisper modely
try:
    from faster_whisper import WhisperModel
    print("Stahov치n칤 Whisper tiny...")
    model = WhisperModel("tiny", device="cpu", compute_type="int8")
    print("Stahov치n칤 Whisper base...")
    model = WhisperModel("base", device="cpu", compute_type="int8")
    print("Whisper modely sta쬰ny!")
except ImportError:
    print("faster-whisper nen칤 nainstalov치n")
except Exception as e:
    print(f"Error: {e}")
PYTHON
WHISPER
    ok "Whisper modely sta쬰ny"
fi

#===============================================================================
# YOLO MODELY
#===============================================================================
log "Stahov치n칤 YOLO model콢..."

if [ -d "$HOME_REAL/mambaforge" ]; then
    sudo -u "$USER_REAL" bash << 'YOLO'
source ~/mambaforge/etc/profile.d/conda.sh
conda activate ai

python << 'PYTHON'
from ultralytics import YOLO

models = ["yolov8n.pt", "yolov8s.pt"]  # nano a small

for m in models:
    print(f"Stahov치n칤 {m}...")
    model = YOLO(m)

print("YOLO modely sta쬰ny!")
PYTHON
YOLO
    ok "YOLO modely sta쬰ny"
fi

#===============================================================================
# SPACY MODELY
#===============================================================================
log "Stahov치n칤 SpaCy model콢..."

if [ -d "$HOME_REAL/mambaforge" ]; then
    sudo -u "$USER_REAL" bash << 'SPACY'
source ~/mambaforge/etc/profile.d/conda.sh
conda activate ai

python -m spacy download en_core_web_sm
python -m spacy download en_core_web_md
SPACY
    ok "SpaCy modely sta쬰ny"
fi

#===============================================================================
# P콎EHLED
#===============================================================================
ok "========== STAHOV츼N칈 DOKON캛ENO =========="
echo ""
log "Sta쬰n칠 modely:"
echo ""
echo "OLLAMA (pro chat a generov치n칤):"
echo "  llama3.2, mistral, phi3, gemma2:2b"
echo "  codellama, deepseek-coder"
echo "  nomic-embed-text (embeddings)"
echo "  llava:7b (vision)"
echo ""
echo "HUGGING FACE (transformers):"
echo "  distilbert, distilgpt2, phi-2"
echo "  sentence-transformers embeddings"
echo "  Helsinki-NLP p콏eklady"
echo ""
echo "WHISPER (speech-to-text):"
echo "  tiny, base"
echo ""
echo "YOLO (detekce objekt콢):"
echo "  yolov8n, yolov8s"
echo ""
echo "SPACY (NLP):"
echo "  en_core_web_sm, en_core_web_md"
echo ""
log "V코e b캩쮂 lok치ln캩 bez API kl칤캜콢! 游"

