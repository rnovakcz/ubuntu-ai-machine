#!/bin/bash
#===============================================================================
# 06-learning-path.sh - AI Developer Learning Path
# VÅ¡e lokÃ¡lnÄ› spustitelnÃ© bez placenÃ½ch ÃºÄtÅ¯
#===============================================================================

set -e

BLUE='\033[0;34m'; GREEN='\033[0;32m'; NC='\033[0m'
log() { echo -e "${BLUE}[INFO]${NC} $1"; }
ok() { echo -e "${GREEN}[OK]${NC} $1"; }

[[ $EUID -ne 0 ]] && { echo "SpusÅ¥te jako root (sudo)"; exit 1; }
USER_REAL=${SUDO_USER:-$USER}
HOME_REAL=$(getent passwd "$USER_REAL" | cut -d: -f6)
LEARN="$HOME_REAL/AI-Learning"

log "========== AI DEVELOPER LEARNING PATH =========="

sudo -u "$USER_REAL" mkdir -p "$LEARN"

#===============================================================================
# 01 - PYTHON PRO AI
#===============================================================================
sudo -u "$USER_REAL" mkdir -p "$LEARN/01-python-ai"
cat > "$LEARN/01-python-ai/README.md" << 'EOF'
# 01 - Python pro AI

## Co se nauÄÃ­Å¡
- NumPy pro numerickÃ© vÃ½poÄty
- Pandas pro prÃ¡ci s daty
- Matplotlib/Seaborn pro vizualizace

## Soubory
- `numpy_basics.py` - ZÃ¡klady NumPy
- `pandas_basics.py` - ZÃ¡klady Pandas
- `visualization.py` - Vizualizace dat
EOF

cat > "$LEARN/01-python-ai/numpy_basics.py" << 'EOF'
#!/usr/bin/env python3
"""NumPy zÃ¡klady pro AI."""
import numpy as np

print("=== NumPy ZÃ¡klady ===\n")

# VytvoÅ™enÃ­ arrays
arr = np.array([1, 2, 3, 4, 5])
print(f"1D Array: {arr}")

matrix = np.array([[1, 2, 3], [4, 5, 6]])
print(f"2D Matrix:\n{matrix}")

# SpeciÃ¡lnÃ­ arrays
zeros = np.zeros((3, 3))
ones = np.ones((2, 4))
random = np.random.randn(3, 3)  # NormÃ¡lnÃ­ distribuce
print(f"\nRandom matrix:\n{random}")

# Operace
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
print(f"\nSÄÃ­tÃ¡nÃ­: {a + b}")
print(f"NÃ¡sobenÃ­: {a * b}")
print(f"Dot product: {np.dot(a, b)}")

# Reshaping - dÅ¯leÅ¾itÃ© pro neuronovÃ© sÃ­tÄ›
data = np.arange(12)
reshaped = data.reshape(3, 4)
print(f"\nReshaped (3x4):\n{reshaped}")

# Indexing a slicing
print(f"\nPrvnÃ­ Å™Ã¡dek: {reshaped[0]}")
print(f"Sloupec 1: {reshaped[:, 1]}")

# Broadcasting - automatickÃ© rozÅ¡Ã­Å™enÃ­ dimenzÃ­
matrix = np.ones((3, 3))
vector = np.array([1, 2, 3])
result = matrix + vector
print(f"\nBroadcasting:\n{result}")

# Statistika
data = np.random.randn(1000)
print(f"\nStatistika:")
print(f"Mean: {np.mean(data):.4f}")
print(f"Std: {np.std(data):.4f}")
print(f"Min: {np.min(data):.4f}")
print(f"Max: {np.max(data):.4f}")
EOF

cat > "$LEARN/01-python-ai/pandas_basics.py" << 'EOF'
#!/usr/bin/env python3
"""Pandas zÃ¡klady pro AI."""
import pandas as pd
import numpy as np

print("=== Pandas ZÃ¡klady ===\n")

# DataFrame vytvoÅ™enÃ­
df = pd.DataFrame({
    'jmeno': ['Alice', 'Bob', 'Charlie', 'Diana'],
    'vek': [25, 30, 35, 28],
    'plat': [50000, 60000, 75000, 55000],
    'oddeleni': ['IT', 'HR', 'IT', 'Marketing']
})
print("DataFrame:")
print(df)
print()

# ZÃ¡kladnÃ­ info
print(f"Shape: {df.shape}")
print(f"Columns: {list(df.columns)}")
print(f"\nInfo:")
print(df.info())
print(f"\nStatistika:")
print(df.describe())

# Selekce
print(f"\nSloupec 'jmeno':\n{df['jmeno']}")
print(f"\nÅ˜Ã¡dky kde vek > 28:\n{df[df['vek'] > 28]}")

# Groupby - velmi uÅ¾iteÄnÃ© pro analÃ½zu
print(f"\nPrÅ¯mÄ›rnÃ½ plat podle oddÄ›lenÃ­:")
print(df.groupby('oddeleni')['plat'].mean())

# PrÃ¡ce s chybÄ›jÃ­cÃ­mi hodnotami
df_missing = df.copy()
df_missing.loc[1, 'plat'] = np.nan
print(f"\nChybÄ›jÃ­cÃ­ hodnoty:\n{df_missing.isnull().sum()}")
df_filled = df_missing.fillna(df_missing['plat'].mean())
print(f"Po vyplnÄ›nÃ­:\n{df_filled}")

# UklÃ¡dÃ¡nÃ­ a naÄÃ­tÃ¡nÃ­
df.to_csv('/tmp/test_data.csv', index=False)
loaded = pd.read_csv('/tmp/test_data.csv')
print(f"\nNaÄteno z CSV:\n{loaded}")
EOF

cat > "$LEARN/01-python-ai/visualization.py" << 'EOF'
#!/usr/bin/env python3
"""Vizualizace dat pro AI."""
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

print("=== Vizualizace ===")
print("Grafy se uloÅ¾Ã­ do /tmp/\n")

# Data
np.random.seed(42)
x = np.linspace(0, 10, 100)
y1 = np.sin(x)
y2 = np.cos(x)

# Line plot
plt.figure(figsize=(10, 6))
plt.plot(x, y1, label='sin(x)', color='blue')
plt.plot(x, y2, label='cos(x)', color='red')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Sinusovky')
plt.legend()
plt.grid(True)
plt.savefig('/tmp/line_plot.png', dpi=150)
plt.close()
print("UloÅ¾eno: /tmp/line_plot.png")

# Histogram
data = np.random.randn(1000)
plt.figure(figsize=(10, 6))
plt.hist(data, bins=30, edgecolor='black', alpha=0.7)
plt.xlabel('Hodnota')
plt.ylabel('ÄŒetnost')
plt.title('Histogram normÃ¡lnÃ­ho rozdÄ›lenÃ­')
plt.savefig('/tmp/histogram.png', dpi=150)
plt.close()
print("UloÅ¾eno: /tmp/histogram.png")

# Seaborn heatmap (confusion matrix style)
matrix = np.random.rand(5, 5)
plt.figure(figsize=(8, 6))
sns.heatmap(matrix, annot=True, fmt='.2f', cmap='Blues')
plt.title('Heatmap')
plt.savefig('/tmp/heatmap.png', dpi=150)
plt.close()
print("UloÅ¾eno: /tmp/heatmap.png")

# Scatter plot
x = np.random.randn(100)
y = x + np.random.randn(100) * 0.5
plt.figure(figsize=(8, 6))
plt.scatter(x, y, alpha=0.6)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Scatter Plot')
plt.savefig('/tmp/scatter.png', dpi=150)
plt.close()
print("UloÅ¾eno: /tmp/scatter.png")

print("\nHotovo! OtevÅ™i obrÃ¡zky v /tmp/")
EOF

#===============================================================================
# 02 - PYTORCH ZÃKLADY
#===============================================================================
sudo -u "$USER_REAL" mkdir -p "$LEARN/02-pytorch-basics"
cat > "$LEARN/02-pytorch-basics/README.md" << 'EOF'
# 02 - PyTorch ZÃ¡klady

## Co se nauÄÃ­Å¡
- Tensory a GPU akcelerace
- Autograd (automatickÃ¡ derivace)
- ZÃ¡kladnÃ­ neuronovÃ¡ sÃ­Å¥

## Soubory
- `tensors.py` - PrÃ¡ce s tensory
- `autograd.py` - AutomatickÃ¡ derivace
- `simple_nn.py` - PrvnÃ­ neuronovÃ¡ sÃ­Å¥
EOF

cat > "$LEARN/02-pytorch-basics/tensors.py" << 'EOF'
#!/usr/bin/env python3
"""PyTorch tensory."""
import torch

print("=== PyTorch Tensory ===\n")
print(f"PyTorch verze: {torch.__version__}")
print(f"CUDA dostupnÃ¡: {torch.cuda.is_available()}")

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"PouÅ¾Ã­vÃ¡m: {device}\n")

# VytvoÅ™enÃ­ tensorÅ¯
t1 = torch.tensor([1, 2, 3, 4])
print(f"Tensor: {t1}")

t2 = torch.zeros(3, 3)
print(f"Zeros:\n{t2}")

t3 = torch.randn(3, 3)  # NormÃ¡lnÃ­ rozdÄ›lenÃ­
print(f"Random:\n{t3}")

# GPU tensor
if torch.cuda.is_available():
    gpu_tensor = torch.randn(1000, 1000, device='cuda')
    print(f"\nGPU tensor shape: {gpu_tensor.shape}")
    print(f"GPU tensor device: {gpu_tensor.device}")

# Operace
a = torch.tensor([1.0, 2.0, 3.0])
b = torch.tensor([4.0, 5.0, 6.0])
print(f"\nSÄÃ­tÃ¡nÃ­: {a + b}")
print(f"NÃ¡sobenÃ­: {a * b}")
print(f"Dot product: {torch.dot(a, b)}")

# Matrix multiplication
m1 = torch.randn(3, 4)
m2 = torch.randn(4, 5)
result = torch.mm(m1, m2)
print(f"\nMatrix mul (3x4) @ (4x5) = {result.shape}")

# Reshaping
t = torch.arange(12)
print(f"\nOriginal: {t}")
print(f"Reshaped (3x4):\n{t.reshape(3, 4)}")
print(f"View (2x6):\n{t.view(2, 6)}")

# DÅ¯leÅ¾itÃ© pro batch processing
batch = torch.randn(32, 3, 224, 224)  # batch, channels, height, width
print(f"\nImage batch shape: {batch.shape}")
EOF

cat > "$LEARN/02-pytorch-basics/autograd.py" << 'EOF'
#!/usr/bin/env python3
"""PyTorch Autograd - automatickÃ¡ derivace."""
import torch

print("=== Autograd ===\n")

# Tensor s requires_grad=True sleduje operace
x = torch.tensor([2.0, 3.0], requires_grad=True)
print(f"x = {x}")

# VÃ½poÄet
y = x ** 2 + 3 * x + 1
print(f"y = xÂ² + 3x + 1 = {y}")

# ZpÄ›tnÃ¡ propagace
z = y.sum()  # SkalÃ¡r pro backward
z.backward()

# Gradienty: dy/dx = 2x + 3
print(f"Gradienty (dy/dx = 2x + 3): {x.grad}")
# Pro x=2: 2*2+3=7, pro x=3: 2*3+3=9 âœ“

# PraktickÃ½ pÅ™Ã­klad: LineÃ¡rnÃ­ regrese
print("\n=== Mini LineÃ¡rnÃ­ Regrese ===")
torch.manual_seed(42)

# Data
X = torch.randn(100, 1)
y_true = 3 * X + 2 + torch.randn(100, 1) * 0.1

# Parametry
w = torch.randn(1, requires_grad=True)
b = torch.randn(1, requires_grad=True)

learning_rate = 0.1

for epoch in range(100):
    # Forward
    y_pred = X * w + b
    loss = ((y_pred - y_true) ** 2).mean()
    
    # Backward
    loss.backward()
    
    # Update (bez gradientÅ¯)
    with torch.no_grad():
        w -= learning_rate * w.grad
        b -= learning_rate * b.grad
    
    # Reset gradientÅ¯
    w.grad.zero_()
    b.grad.zero_()
    
    if epoch % 20 == 0:
        print(f"Epoch {epoch}: loss={loss.item():.4f}, w={w.item():.4f}, b={b.item():.4f}")

print(f"\nNauÄenÃ©: w={w.item():.2f} (skuteÄnÃ© 3), b={b.item():.2f} (skuteÄnÃ© 2)")
EOF

cat > "$LEARN/02-pytorch-basics/simple_nn.py" << 'EOF'
#!/usr/bin/env python3
"""PrvnÃ­ neuronovÃ¡ sÃ­Å¥ v PyTorch."""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

print("=== NeuronovÃ¡ SÃ­Å¥ ===\n")

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Device: {device}")

# GenerovÃ¡nÃ­ dat (XOR problÃ©m - nelze Å™eÅ¡it lineÃ¡rnÄ›)
torch.manual_seed(42)
X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)
y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)

# Definice sÃ­tÄ›
class SimpleNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(2, 8)
        self.layer2 = nn.Linear(8, 1)
        self.activation = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        x = self.activation(self.layer1(x))
        x = self.sigmoid(self.layer2(x))
        return x

# Model, loss, optimizer
model = SimpleNN().to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.1)

print(f"Model:\n{model}\n")

# Training
X, y = X.to(device), y.to(device)

for epoch in range(1000):
    # Forward
    outputs = model(X)
    loss = criterion(outputs, y)
    
    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if epoch % 200 == 0:
        print(f"Epoch {epoch}: loss={loss.item():.4f}")

# TestovÃ¡nÃ­
print("\n=== VÃ½sledky ===")
model.eval()
with torch.no_grad():
    predictions = model(X)
    for i in range(len(X)):
        pred = predictions[i].item()
        actual = y[i].item()
        print(f"Input: {X[i].tolist()} -> Pred: {pred:.3f}, Actual: {actual}")
EOF

#===============================================================================
# 03 - TRANSFORMERS & LLM
#===============================================================================
sudo -u "$USER_REAL" mkdir -p "$LEARN/03-transformers-llm"
cat > "$LEARN/03-transformers-llm/README.md" << 'EOF'
# 03 - Transformers & LLM

## Co se nauÄÃ­Å¡
- Hugging Face transformers
- PrÃ¡ce s lokÃ¡lnÃ­mi LLM
- Text generation, embeddings

## Soubory
- `hf_basics.py` - Hugging Face zÃ¡klady
- `local_llm.py` - LokÃ¡lnÃ­ LLM s Ollama
- `embeddings.py` - Text embeddings
EOF

cat > "$LEARN/03-transformers-llm/hf_basics.py" << 'EOF'
#!/usr/bin/env python3
"""Hugging Face Transformers zÃ¡klady."""
from transformers import pipeline, AutoTokenizer, AutoModel
import torch

print("=== Hugging Face Transformers ===\n")

# 1. Sentiment Analysis (stÃ¡hne malÃ½ model)
print("1. Sentiment Analysis")
classifier = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")
results = classifier([
    "I love this product!",
    "This is terrible.",
    "It's okay, nothing special."
])
for r in results:
    print(f"  {r['label']}: {r['score']:.3f}")

# 2. Text Generation (malÃ½ GPT-2)
print("\n2. Text Generation")
generator = pipeline("text-generation", model="distilgpt2")
text = generator("Artificial intelligence is", max_length=30, num_return_sequences=1)
print(f"  {text[0]['generated_text']}")

# 3. Question Answering
print("\n3. Question Answering")
qa = pipeline("question-answering", model="distilbert-base-cased-distilled-squad")
context = "Python is a programming language created by Guido van Rossum in 1991."
question = "Who created Python?"
answer = qa(question=question, context=context)
print(f"  Q: {question}")
print(f"  A: {answer['answer']} (score: {answer['score']:.3f})")

# 4. Fill Mask
print("\n4. Fill Mask")
fill = pipeline("fill-mask", model="distilbert-base-uncased")
result = fill("Machine learning is a type of [MASK] intelligence.")
print(f"  Top prediction: {result[0]['token_str']} ({result[0]['score']:.3f})")

# 5. Named Entity Recognition
print("\n5. Named Entity Recognition")
ner = pipeline("ner", model="dslim/bert-base-NER", aggregation_strategy="simple")
entities = ner("Bill Gates founded Microsoft in Seattle.")
for e in entities:
    print(f"  {e['word']}: {e['entity_group']}")

print("\nâœ“ VÅ¡echny modely bÄ›Å¾Ã­ lokÃ¡lnÄ›!")
EOF

cat > "$LEARN/03-transformers-llm/local_llm.py" << 'EOF'
#!/usr/bin/env python3
"""LokÃ¡lnÃ­ LLM s Ollama (bez API klÃ­ÄÅ¯)."""
import requests
import json

OLLAMA_URL = "http://localhost:11434"

def check_ollama():
    """Zkontroluj jestli Ollama bÄ›Å¾Ã­."""
    try:
        r = requests.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        return r.status_code == 200
    except:
        return False

def list_models():
    """Seznam lokÃ¡lnÃ­ch modelÅ¯."""
    r = requests.get(f"{OLLAMA_URL}/api/tags")
    models = r.json().get("models", [])
    return [m["name"] for m in models]

def generate(model: str, prompt: str) -> str:
    """GenerovÃ¡nÃ­ textu."""
    r = requests.post(
        f"{OLLAMA_URL}/api/generate",
        json={"model": model, "prompt": prompt, "stream": False}
    )
    return r.json()["response"]

def chat(model: str, messages: list) -> str:
    """Chat s modelem."""
    r = requests.post(
        f"{OLLAMA_URL}/api/chat",
        json={"model": model, "messages": messages, "stream": False}
    )
    return r.json()["message"]["content"]

if __name__ == "__main__":
    print("=== LokÃ¡lnÃ­ LLM s Ollama ===\n")
    
    if not check_ollama():
        print("âŒ Ollama nebÄ›Å¾Ã­!")
        print("SpusÅ¥te: ollama serve")
        print("StÃ¡hnÄ›te model: ollama pull llama3.2")
        exit(1)
    
    models = list_models()
    print(f"DostupnÃ© modely: {models}")
    
    if not models:
        print("\nâŒ Å½Ã¡dnÃ© modely! StÃ¡hnÄ›te: ollama pull llama3.2")
        exit(1)
    
    model = models[0]
    print(f"\nPouÅ¾Ã­vÃ¡m model: {model}")
    
    # JednoduchÃ¡ generace
    print("\n--- GenerovÃ¡nÃ­ ---")
    prompt = "VysvÄ›tli co je neuronovÃ¡ sÃ­Å¥ ve 2 vÄ›tÃ¡ch."
    response = generate(model, prompt)
    print(f"Q: {prompt}")
    print(f"A: {response}")
    
    # Chat
    print("\n--- Chat ---")
    messages = [
        {"role": "user", "content": "Ahoj! Jak se mÃ¡Å¡?"},
    ]
    response = chat(model, messages)
    print(f"User: {messages[0]['content']}")
    print(f"AI: {response}")
EOF

cat > "$LEARN/03-transformers-llm/embeddings.py" << 'EOF'
#!/usr/bin/env python3
"""Text Embeddings - zÃ¡klad pro RAG."""
from sentence_transformers import SentenceTransformer
import numpy as np

print("=== Text Embeddings ===\n")

# MalÃ½ ale kvalitnÃ­ model
model = SentenceTransformer('all-MiniLM-L6-v2')
print(f"Model: all-MiniLM-L6-v2")
print(f"Embedding size: 384\n")

# Dokumenty
documents = [
    "Python is a programming language used for AI.",
    "Machine learning uses data to train models.",
    "Deep learning is based on neural networks.",
    "Cats are popular pets.",
    "Dogs are loyal companions."
]

# VytvoÅ™enÃ­ embeddingÅ¯
embeddings = model.encode(documents)
print(f"Embeddings shape: {embeddings.shape}")

# Query
query = "What is artificial intelligence?"
query_embedding = model.encode(query)

# VÃ½poÄet podobnosti (cosine similarity)
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

print(f"\nQuery: '{query}'")
print("\nPodobnosti:")

similarities = []
for i, doc in enumerate(documents):
    sim = cosine_similarity(query_embedding, embeddings[i])
    similarities.append((sim, doc))

# SeÅ™azenÃ­ podle podobnosti
similarities.sort(reverse=True)
for sim, doc in similarities:
    print(f"  {sim:.3f}: {doc}")

print("\nâœ“ NejpodobnÄ›jÅ¡Ã­ dokumenty jsou o AI/ML!")
EOF

#===============================================================================
# 04 - RAG SYSTÃ‰M
#===============================================================================
sudo -u "$USER_REAL" mkdir -p "$LEARN/04-rag-system"
cat > "$LEARN/04-rag-system/README.md" << 'EOF'
# 04 - RAG SystÃ©m

## Co se nauÄÃ­Å¡
- Retrieval Augmented Generation
- Vector store (ChromaDB)
- LangChain pro RAG

## Soubory
- `simple_rag.py` - JednoduchÃ½ RAG systÃ©m
- `chroma_basics.py` - ChromaDB vector store
EOF

cat > "$LEARN/04-rag-system/simple_rag.py" << 'EOF'
#!/usr/bin/env python3
"""JednoduchÃ½ RAG systÃ©m - vÅ¡e lokÃ¡lnÄ›."""
from sentence_transformers import SentenceTransformer
import chromadb
import requests

print("=== RAG SystÃ©m ===\n")

# 1. Embedding model
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# 2. Vector store
client = chromadb.Client()
collection = client.create_collection("documents")

# 3. NaÅ¡e znalostnÃ­ bÃ¡ze
knowledge_base = [
    "Python byl vytvoÅ™en Guido van Rossumem v roce 1991.",
    "PyTorch je framework pro deep learning od Facebooku.",
    "TensorFlow vytvoÅ™il Google v roce 2015.",
    "Transformers architektura byla pÅ™edstavena v paperu 'Attention is All You Need' v 2017.",
    "GPT znamenÃ¡ Generative Pre-trained Transformer.",
    "BERT je model od Google pro porozumÄ›nÃ­ textu.",
    "LLM jsou velkÃ© jazykovÃ© modely trÃ©novanÃ© na miliardÃ¡ch tokenÅ¯.",
    "RAG kombinuje retrieval s generovÃ¡nÃ­m textu.",
    "Vector database uklÃ¡dajÃ­ embeddingy pro rychlÃ© vyhledÃ¡vÃ¡nÃ­.",
    "Fine-tuning adaptuje pÅ™edtrÃ©novanÃ½ model na specifickÃ½ Ãºkol."
]

# 4. IndexovÃ¡nÃ­ dokumentÅ¯
print("Indexuji dokumenty...")
embeddings = embedder.encode(knowledge_base).tolist()
collection.add(
    documents=knowledge_base,
    embeddings=embeddings,
    ids=[f"doc_{i}" for i in range(len(knowledge_base))]
)
print(f"IndexovÃ¡no {len(knowledge_base)} dokumentÅ¯\n")

# 5. RAG funkce
def rag_query(question: str, top_k: int = 3) -> str:
    """RAG: najdi relevantnÃ­ dokumenty a vygeneruj odpovÄ›Ä."""
    # Retrieval
    query_embedding = embedder.encode(question).tolist()
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    
    context = "\n".join(results['documents'][0])
    
    # GenerovÃ¡nÃ­ (s Ollama)
    try:
        prompt = f"""Kontext:
{context}

OtÃ¡zka: {question}

OdpovÄ›z struÄnÄ› na zÃ¡kladÄ› kontextu:"""
        
        r = requests.post(
            "http://localhost:11434/api/generate",
            json={"model": "llama3.2", "prompt": prompt, "stream": False},
            timeout=30
        )
        return r.json()["response"]
    except:
        return f"[LLM nedostupnÃ½]\n\nNalezenÃ½ kontext:\n{context}"

# 6. Test
questions = [
    "Kdo vytvoÅ™il Python?",
    "Co je PyTorch?",
    "Co znamenÃ¡ GPT?"
]

for q in questions:
    print(f"Q: {q}")
    answer = rag_query(q)
    print(f"A: {answer}\n")
EOF

cat > "$LEARN/04-rag-system/chroma_basics.py" << 'EOF'
#!/usr/bin/env python3
"""ChromaDB zÃ¡klady."""
import chromadb
from chromadb.utils import embedding_functions

print("=== ChromaDB ZÃ¡klady ===\n")

# VytvoÅ™enÃ­ klienta (in-memory)
client = chromadb.Client()

# Nebo persistent
# client = chromadb.PersistentClient(path="/tmp/chroma_db")

# Embedding function (Sentence Transformers)
ef = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="all-MiniLM-L6-v2"
)

# VytvoÅ™enÃ­ kolekce
collection = client.create_collection(
    name="my_collection",
    embedding_function=ef
)

# PÅ™idÃ¡nÃ­ dokumentÅ¯
documents = [
    "The quick brown fox jumps over the lazy dog.",
    "Machine learning is transforming industries.",
    "Python is great for data science.",
    "Neural networks can learn complex patterns.",
]

collection.add(
    documents=documents,
    metadatas=[{"source": f"doc_{i}"} for i in range(len(documents))],
    ids=[f"id_{i}" for i in range(len(documents))]
)

print(f"DokumentÅ¯ v kolekci: {collection.count()}")

# VyhledÃ¡vÃ¡nÃ­
print("\n--- VyhledÃ¡vÃ¡nÃ­ ---")
results = collection.query(
    query_texts=["What is AI?"],
    n_results=2
)

print("Query: 'What is AI?'")
print("NejpodobnÄ›jÅ¡Ã­ dokumenty:")
for doc, dist in zip(results['documents'][0], results['distances'][0]):
    print(f"  [{dist:.3f}] {doc}")

# FiltrovÃ¡nÃ­ podle metadat
print("\n--- FiltrovÃ¡nÃ­ ---")
results = collection.query(
    query_texts=["programming"],
    n_results=2,
    where={"source": "doc_2"}
)
print(f"S filtrem: {results['documents']}")

# Update
collection.update(
    ids=["id_0"],
    documents=["Updated: A quick fox jumped."]
)
print("\nDokument id_0 aktualizovÃ¡n")

# Delete
collection.delete(ids=["id_3"])
print(f"DokumentÅ¯ po smazÃ¡nÃ­: {collection.count()}")
EOF

#===============================================================================
# 05 - FINE-TUNING
#===============================================================================
sudo -u "$USER_REAL" mkdir -p "$LEARN/05-fine-tuning"
cat > "$LEARN/05-fine-tuning/README.md" << 'EOF'
# 05 - Fine-tuning

## Co se nauÄÃ­Å¡
- LoRA a QLoRA techniky
- PEFT knihovna
- PÅ™Ã­prava datasetu

## Soubory
- `lora_intro.py` - Ãšvod do LoRA
- `prepare_dataset.py` - PÅ™Ã­prava dat pro fine-tuning
EOF

cat > "$LEARN/05-fine-tuning/lora_intro.py" << 'EOF'
#!/usr/bin/env python3
"""LoRA (Low-Rank Adaptation) Ãºvod."""
import torch
import torch.nn as nn
from peft import LoraConfig, get_peft_model, TaskType
from transformers import AutoModelForSequenceClassification, AutoTokenizer

print("=== LoRA Fine-tuning ===\n")

# ProÄ LoRA?
print("""
LoRA (Low-Rank Adaptation):
- MÃ­sto fine-tuningu vÅ¡ech vah trÃ©nujeme pouze malÃ© "adaptÃ©ry"
- Drasticky sniÅ¾uje poÄet trÃ©novatelnÃ½ch parametrÅ¯
- Å etÅ™Ã­ pamÄ›Å¥ GPU
- RychlejÅ¡Ã­ trÃ©nink
- Lze kombinovat vÃ­ce LoRA adaptÃ©rÅ¯
""")

# PÅ™Ã­klad s malÃ½m modelem
model_name = "distilbert-base-uncased"
print(f"Model: {model_name}")

# NaÄtenÃ­ modelu
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2
)

# PoÄet parametrÅ¯ pÅ™ed LoRA
total_params = sum(p.numel() for p in model.parameters())
print(f"Celkem parametrÅ¯: {total_params:,}")

# LoRA konfigurace
lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    r=8,              # Rank - niÅ¾Å¡Ã­ = mÃ©nÄ› parametrÅ¯
    lora_alpha=32,    # Scaling faktor
    lora_dropout=0.1,
    target_modules=["q_lin", "v_lin"]  # KterÃ© vrstvy adaptovat
)

# Aplikace LoRA
peft_model = get_peft_model(model, lora_config)

# Statistiky
print("\n--- LoRA Statistiky ---")
peft_model.print_trainable_parameters()

# UloÅ¾enÃ­ pouze LoRA vah
# peft_model.save_pretrained("./lora_adapter")
# Velikost: ~MB mÃ­sto GB!

print("""
DalÅ¡Ã­ kroky pro plnÃ½ fine-tuning:
1. PÅ™iprav dataset (viz prepare_dataset.py)
2. PouÅ¾ij Trainer z transformers
3. TrÃ©nuj s malÃ½m learning rate (1e-4 aÅ¾ 5e-5)
4. UloÅ¾ pouze LoRA adaptÃ©r
""")
EOF

cat > "$LEARN/05-fine-tuning/prepare_dataset.py" << 'EOF'
#!/usr/bin/env python3
"""PÅ™Ã­prava datasetu pro fine-tuning."""
from datasets import Dataset, load_dataset
import json

print("=== PÅ™Ã­prava Datasetu ===\n")

# 1. VytvoÅ™enÃ­ vlastnÃ­ho datasetu
print("1. VlastnÃ­ dataset")
data = {
    "text": [
        "This product is amazing!",
        "Terrible experience, would not recommend.",
        "It's okay, nothing special.",
        "Best purchase ever!",
        "Complete waste of money."
    ],
    "label": [1, 0, 1, 1, 0]  # 1=positive, 0=negative
}

dataset = Dataset.from_dict(data)
print(f"   VytvoÅ™eno: {len(dataset)} pÅ™Ã­kladÅ¯")
print(f"   Sloupce: {dataset.column_names}")

# 2. FormÃ¡t pro instruktÃ¡Å¾nÃ­ fine-tuning (chat)
print("\n2. InstruktÃ¡Å¾nÃ­ formÃ¡t")
instruction_data = [
    {
        "instruction": "Classify the sentiment of this review.",
        "input": "This product is amazing!",
        "output": "Positive"
    },
    {
        "instruction": "Classify the sentiment of this review.",
        "input": "Terrible experience.",
        "output": "Negative"
    }
]

# UloÅ¾enÃ­ jako JSONL
with open("/tmp/train_data.jsonl", "w") as f:
    for item in instruction_data:
        f.write(json.dumps(item) + "\n")
print("   UloÅ¾eno: /tmp/train_data.jsonl")

# 3. NaÄtenÃ­ veÅ™ejnÃ©ho datasetu
print("\n3. VeÅ™ejnÃ© datasety z Hugging Face")
try:
    # MalÃ½ dataset pro ukÃ¡zku
    imdb = load_dataset("imdb", split="train[:100]")
    print(f"   IMDB: {len(imdb)} pÅ™Ã­kladÅ¯")
    print(f"   PÅ™Ã­klad: {imdb[0]['text'][:100]}...")
except:
    print("   (VyÅ¾aduje internet)")

# 4. FormÃ¡tovÃ¡nÃ­ pro rÅ¯znÃ© modely
print("\n4. FormÃ¡ty pro rÅ¯znÃ© modely")

# Alpaca formÃ¡t
alpaca_template = """Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{input}

### Response:
{output}"""

# ChatML formÃ¡t
chatml_template = """<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
{instruction}
{input}<|im_end|>
<|im_start|>assistant
{output}<|im_end|>"""

print("   Alpaca formÃ¡t: pro Llama, Mistral")
print("   ChatML formÃ¡t: pro modely s chat template")

# PÅ™Ã­klad konverze
example = instruction_data[0]
print(f"\n--- Alpaca pÅ™Ã­klad ---")
print(alpaca_template.format(**example))
EOF

#===============================================================================
# 06 - COMPUTER VISION
#===============================================================================
sudo -u "$USER_REAL" mkdir -p "$LEARN/06-computer-vision"
cat > "$LEARN/06-computer-vision/README.md" << 'EOF'
# 06 - Computer Vision

## Co se nauÄÃ­Å¡
- ZpracovÃ¡nÃ­ obrÃ¡zkÅ¯
- CNN a klasifikace
- Detekce objektÅ¯ s YOLO

## Soubory
- `image_basics.py` - ZÃ¡klady zpracovÃ¡nÃ­ obrÃ¡zkÅ¯
- `simple_cnn.py` - CNN pro klasifikaci
- `yolo_detection.py` - YOLO detekce
EOF

cat > "$LEARN/06-computer-vision/image_basics.py" << 'EOF'
#!/usr/bin/env python3
"""ZÃ¡klady zpracovÃ¡nÃ­ obrÃ¡zkÅ¯."""
import torch
import torchvision.transforms as transforms
from PIL import Image
import numpy as np

print("=== ZpracovÃ¡nÃ­ ObrÃ¡zkÅ¯ ===\n")

# VytvoÅ™enÃ­ testovacÃ­ho obrÃ¡zku
img_array = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)
img = Image.fromarray(img_array)
img.save("/tmp/test_image.png")
print("VytvoÅ™en testovacÃ­ obrÃ¡zek: /tmp/test_image.png")

# Transformace pro neuronovÃ© sÃ­tÄ›
transform = transforms.Compose([
    transforms.Resize((224, 224)),      # StandardnÃ­ velikost pro CNN
    transforms.ToTensor(),               # PÅ™evod na tensor [0,1]
    transforms.Normalize(                # ImageNet normalizace
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# Aplikace transformace
img = Image.open("/tmp/test_image.png")
tensor = transform(img)
print(f"\nTransformovanÃ½ tensor:")
print(f"  Shape: {tensor.shape}")  # [3, 224, 224]
print(f"  Min: {tensor.min():.2f}, Max: {tensor.max():.2f}")

# Batch pro sÃ­Å¥
batch = tensor.unsqueeze(0)  # PÅ™idÃ¡nÃ­ batch dimenze
print(f"  Batch shape: {batch.shape}")  # [1, 3, 224, 224]

# Data augmentace
aug_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    transforms.ToTensor()
])

print("\nData augmentace:")
print("  - RandomHorizontalFlip")
print("  - RandomRotation")
print("  - ColorJitter")
print("  - RandomResizedCrop")

# UkÃ¡zka vÃ­ce augmentacÃ­
img = Image.open("/tmp/test_image.png")
for i in range(3):
    augmented = aug_transform(img)
    print(f"  Augmented {i+1}: {augmented.shape}")
EOF

cat > "$LEARN/06-computer-vision/simple_cnn.py" << 'EOF'
#!/usr/bin/env python3
"""CNN pro klasifikaci obrÃ¡zkÅ¯."""
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

print("=== CNN Klasifikace ===\n")

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Device: {device}")

# Transformace
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# MNIST dataset (automaticky se stÃ¡hne)
print("\nStahuji MNIST dataset...")
train_dataset = datasets.MNIST(
    root='/tmp/data', train=True, download=True, transform=transform
)
test_dataset = datasets.MNIST(
    root='/tmp/data', train=False, download=True, transform=transform
)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
print(f"Train: {len(train_dataset)}, Test: {len(test_dataset)}")

# CNN Model
class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.MaxPool2d(2)
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.25)
    
    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))  # 28->14
        x = self.pool(self.relu(self.conv2(x)))  # 14->7
        x = x.view(-1, 64 * 7 * 7)
        x = self.dropout(self.relu(self.fc1(x)))
        x = self.fc2(x)
        return x

model = SimpleCNN().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

print(f"\nModel architecture:\n{model}")

# Training (jen 2 epochy pro ukÃ¡zku)
print("\nTrÃ©nink...")
for epoch in range(2):
    model.train()
    total_loss = 0
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        if batch_idx % 200 == 0:
            print(f"  Epoch {epoch+1}, Batch {batch_idx}: loss={loss.item():.4f}")
    
    print(f"Epoch {epoch+1} avg loss: {total_loss/len(train_loader):.4f}")

# Evaluace
model.eval()
correct = 0
with torch.no_grad():
    for data, target in test_loader:
        data, target = data.to(device), target.to(device)
        output = model(data)
        pred = output.argmax(dim=1)
        correct += (pred == target).sum().item()

accuracy = 100 * correct / len(test_dataset)
print(f"\nTest accuracy: {accuracy:.2f}%")
EOF

cat > "$LEARN/06-computer-vision/yolo_detection.py" << 'EOF'
#!/usr/bin/env python3
"""YOLO detekce objektÅ¯."""
from ultralytics import YOLO
import numpy as np
from PIL import Image

print("=== YOLO Detekce ===\n")

# StaÅ¾enÃ­ YOLOv8 nano (nejmenÅ¡Ã­, rychlÃ½)
print("NaÄÃ­tÃ¡m YOLOv8 nano model...")
model = YOLO('yolov8n.pt')

# VytvoÅ™enÃ­ testovacÃ­ho obrÃ¡zku
img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
Image.fromarray(img).save('/tmp/test_detect.jpg')

# Detekce
print("SpouÅ¡tÃ­m detekci...")
results = model('/tmp/test_detect.jpg')

# VÃ½sledky
print(f"\nVÃ½sledky detekce:")
for r in results:
    boxes = r.boxes
    print(f"  Nalezeno objektÅ¯: {len(boxes)}")
    
    if len(boxes) > 0:
        for box in boxes:
            cls = int(box.cls[0])
            conf = float(box.conf[0])
            name = model.names[cls]
            print(f"    - {name}: {conf:.2f}")

# YOLO tÅ™Ã­dy
print(f"\nYOLO rozpoznÃ¡vÃ¡ {len(model.names)} tÅ™Ã­d:")
print(f"  {list(model.names.values())[:10]}...")

# Pro skuteÄnÃ© pouÅ¾itÃ­:
print("""
Pro detekci na vlastnÃ­ch obrÃ¡zcÃ­ch:
    results = model('your_image.jpg')
    results[0].save('result.jpg')  # UloÅ¾Ã­ s bounding boxy

Pro video:
    results = model('video.mp4')

Pro webcam:
    results = model(source=0, show=True)
""")
EOF

#===============================================================================
# 07 - AUDIO & SPEECH
#===============================================================================
sudo -u "$USER_REAL" mkdir -p "$LEARN/07-audio-speech"
cat > "$LEARN/07-audio-speech/README.md" << 'EOF'
# 07 - Audio & Speech

## Co se nauÄÃ­Å¡
- Speech-to-text (Whisper)
- Text-to-speech
- Audio zpracovÃ¡nÃ­

## Soubory
- `whisper_stt.py` - Speech to text
- `audio_basics.py` - ZÃ¡klady zpracovÃ¡nÃ­ zvuku
EOF

cat > "$LEARN/07-audio-speech/whisper_stt.py" << 'EOF'
#!/usr/bin/env python3
"""Whisper Speech-to-Text (lokÃ¡lnÄ›)."""
import torch

print("=== Whisper Speech-to-Text ===\n")

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Device: {device}")

# PouÅ¾ijeme faster-whisper (efektivnÄ›jÅ¡Ã­)
try:
    from faster_whisper import WhisperModel
    
    print("\nNaÄÃ­tÃ¡m Whisper tiny model...")
    model = WhisperModel("tiny", device=device, compute_type="float16" if device=="cuda" else "int8")
    
    print("""
Model naÄten! Pro transkripci:

    segments, info = model.transcribe("audio.mp3")
    for segment in segments:
        print(f"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}")

PodporovanÃ© formÃ¡ty: mp3, wav, m4a, flac, ogg

Modely (vÄ›tÅ¡Ã­ = pÅ™esnÄ›jÅ¡Ã­, pomalejÅ¡Ã­):
    tiny   - 39M params, ~10x realtime
    base   - 74M params, ~7x realtime  
    small  - 244M params, ~4x realtime
    medium - 769M params, ~2x realtime
    large  - 1550M params, ~1x realtime
""")
    
except ImportError:
    print("faster-whisper nenÃ­ nainstalovÃ¡n.")
    print("ZkusÃ­m openai-whisper...")
    
    import whisper
    
    print("\nNaÄÃ­tÃ¡m Whisper tiny model...")
    model = whisper.load_model("tiny", device=device)
    
    print("""
Model naÄten! Pro transkripci:

    result = model.transcribe("audio.mp3")
    print(result["text"])

Pro segmenty s Äasem:
    for segment in result["segments"]:
        print(f"[{segment['start']:.2f}s] {segment['text']}")
""")
EOF

cat > "$LEARN/07-audio-speech/audio_basics.py" << 'EOF'
#!/usr/bin/env python3
"""ZÃ¡klady zpracovÃ¡nÃ­ zvuku."""
import numpy as np
import soundfile as sf

print("=== Audio ZÃ¡klady ===\n")

# GenerovÃ¡nÃ­ testovacÃ­ho zvuku (sinusovka)
sample_rate = 22050
duration = 2.0
frequency = 440  # A4

t = np.linspace(0, duration, int(sample_rate * duration))
audio = 0.5 * np.sin(2 * np.pi * frequency * t)

# UloÅ¾enÃ­
sf.write('/tmp/test_audio.wav', audio, sample_rate)
print(f"VytvoÅ™en testovacÃ­ zvuk: /tmp/test_audio.wav")
print(f"  Sample rate: {sample_rate} Hz")
print(f"  Duration: {duration} s")
print(f"  Frequency: {frequency} Hz (A4)")

# NaÄtenÃ­
data, sr = sf.read('/tmp/test_audio.wav')
print(f"\nNaÄteno:")
print(f"  Shape: {data.shape}")
print(f"  Sample rate: {sr}")
print(f"  Duration: {len(data)/sr:.2f} s")

# Librosa pro pokroÄilejÅ¡Ã­ analÃ½zu
try:
    import librosa
    import librosa.display
    
    # Mel spectrogram
    mel_spec = librosa.feature.melspectrogram(y=data, sr=sr)
    print(f"\nMel spectrogram shape: {mel_spec.shape}")
    
    # MFCC features (uÅ¾iteÄnÃ© pro speech)
    mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=13)
    print(f"MFCC shape: {mfcc.shape}")
    
except ImportError:
    print("\nlibrosa nenÃ­ nainstalovÃ¡na pro pokroÄilÃ© features")

print("""
DalÅ¡Ã­ moÅ¾nosti:
- librosa: analÃ½za, MFCC, spectrogramy
- soundfile: naÄÃ­tÃ¡nÃ­/uklÃ¡dÃ¡nÃ­
- pydub: editace, konverze formÃ¡tÅ¯
- torchaudio: PyTorch integrace
""")
EOF

chown -R "$USER_REAL:$USER_REAL" "$LEARN"

#===============================================================================
# README
#===============================================================================
cat > "$LEARN/README.md" << 'EOF'
# ðŸŽ“ AI Developer Learning Path

KompletnÃ­ prÅ¯vodce pro AI vÃ½vojÃ¡Å™e - vÅ¡e lokÃ¡lnÄ› bez placenÃ½ch ÃºÄtÅ¯!

## Struktura

```
AI-Learning/
â”œâ”€â”€ 01-python-ai/         # Python, NumPy, Pandas, vizualizace
â”œâ”€â”€ 02-pytorch-basics/    # Tensory, autograd, prvnÃ­ sÃ­Å¥
â”œâ”€â”€ 03-transformers-llm/  # Hugging Face, Ollama, embeddings
â”œâ”€â”€ 04-rag-system/        # RAG, ChromaDB, vector search
â”œâ”€â”€ 05-fine-tuning/       # LoRA, PEFT, pÅ™Ã­prava dat
â”œâ”€â”€ 06-computer-vision/   # CNN, YOLO, zpracovÃ¡nÃ­ obrÃ¡zkÅ¯
â””â”€â”€ 07-audio-speech/      # Whisper, audio processing
```

## Jak zaÄÃ­t

```bash
# Aktivuj AI prostÅ™edÃ­
conda activate ai

# PÅ™ejdi do sloÅ¾ky
cd ~/AI-Learning/01-python-ai

# SpusÅ¥ pÅ™Ã­klad
python numpy_basics.py
```

## PoÅ™adÃ­ studia

1. **01-python-ai** - Pokud neznÃ¡Å¡ NumPy/Pandas
2. **02-pytorch-basics** - ZÃ¡klady deep learning
3. **03-transformers-llm** - ModernÃ­ NLP a LLM
4. **04-rag-system** - PraktickÃ© AI aplikace
5. **05-fine-tuning** - Customizace modelÅ¯
6. **06-computer-vision** - PrÃ¡ce s obrÃ¡zky
7. **07-audio-speech** - PrÃ¡ce se zvukem

## PoÅ¾adavky

- Conda environment `ai`
- Pro LLM: bÄ›Å¾Ã­cÃ­ Ollama (`ollama serve`)
- GPU doporuÄeno ale ne nutnÃ©

## LokÃ¡lnÃ­ modely

```bash
# Ollama modely
ollama pull llama3.2      # 2GB, vÅ¡eobecnÃ½
ollama pull codellama     # Pro kÃ³d
ollama pull nomic-embed-text  # Embeddings

# Hugging Face (stahujÃ­ se automaticky)
# distilbert, distilgpt2, all-MiniLM-L6-v2
```

VÅ¡e bÄ›Å¾Ã­ lokÃ¡lnÄ› bez API klÃ­ÄÅ¯! ðŸš€
EOF

ok "========== AI LEARNING PATH VYTVOÅ˜EN =========="
log "SloÅ¾ka: ~/AI-Learning/"
log "ZaÄni: cd ~/AI-Learning/01-python-ai && python numpy_basics.py"

